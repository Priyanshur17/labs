{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import string\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Character set definition\n",
        "all_characters = string.ascii_letters + \" .,;'-!?()\"\n",
        "n_characters = len(all_characters)\n",
        "char_to_index = {ch: i for i, ch in enumerate(all_characters)}\n",
        "\n",
        "def line_to_tensor(line):\n",
        "    \"\"\"Convert string to tensor with correct dimensions\"\"\"\n",
        "    tensor = torch.zeros(1, len(line), n_characters)  # Changed dimensions to (batch, seq, features)\n",
        "    for i, char in enumerate(line):\n",
        "        if char in char_to_index:\n",
        "            tensor[0][i][char_to_index[char]] = 1\n",
        "    return tensor\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # x shape: (batch, seq, feature)\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        # Use the output from the last time step\n",
        "        out = self.fc(out[:, -1, :])  # Changed indexing for batch_first=True\n",
        "        out = self.softmax(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        # Shape: (num_layers * num_directions, batch, hidden_size)\n",
        "        return torch.zeros(1, batch_size, self.hidden_size)\n",
        "\n",
        "def load_eng_fra_data(file_path):\n",
        "    \"\"\"Load English-French data\"\"\"\n",
        "    data = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) == 2:\n",
        "                    data.append((parts[0], 'English'))\n",
        "                    data.append((parts[1], 'French'))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Could not find file {file_path}\")\n",
        "        return []\n",
        "    return data\n",
        "\n",
        "def load_names_data(folder_path):\n",
        "    \"\"\"Load names data\"\"\"\n",
        "    data = []\n",
        "    categories = []\n",
        "\n",
        "    try:\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith(\".txt\"):\n",
        "                category = os.path.splitext(filename)[0]\n",
        "                categories.append(category)\n",
        "\n",
        "                with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:\n",
        "                    names = f.read().strip().split('\\n')\n",
        "                    data.extend([(name, category) for name in names])\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Could not find folder {folder_path}\")\n",
        "        return [], []\n",
        "\n",
        "    return data, categories\n",
        "\n",
        "def random_training_example(data, category_indices):\n",
        "    \"\"\"Generate a random training example\"\"\"\n",
        "    line, category = random.choice(data)\n",
        "    category_tensor = torch.tensor([category_indices.index(category)], dtype=torch.long)\n",
        "    line_tensor = line_to_tensor(line)\n",
        "    return category, line, category_tensor, line_tensor\n",
        "\n",
        "def train(model, data, category_indices, n_iters=1000, lr=0.005, print_every=100):\n",
        "    \"\"\"Train the model\"\"\"\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    total_loss = 0\n",
        "    current_loss = 0\n",
        "    all_losses = []\n",
        "\n",
        "    try:\n",
        "        for iter in range(1, n_iters + 1):\n",
        "            category, line, category_tensor, line_tensor = random_training_example(data, category_indices)\n",
        "\n",
        "            # Initialize hidden state with correct batch size\n",
        "            hidden = model.init_hidden(batch_size=line_tensor.size(0))\n",
        "\n",
        "            model.zero_grad()\n",
        "            output, hidden = model(line_tensor, hidden)\n",
        "\n",
        "            loss = criterion(output, category_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            current_loss += loss.item()\n",
        "\n",
        "            if iter % print_every == 0:\n",
        "                avg_loss = current_loss / print_every\n",
        "                print(f'Iteration: {iter}/{n_iters} | Loss: {avg_loss:.4f} | Example: {line} -> {category}')\n",
        "                all_losses.append(avg_loss)\n",
        "                current_loss = 0\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nTraining interrupted by user\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {str(e)}\")\n",
        "        raise  # Re-raise the exception for debugging\n",
        "\n",
        "    return all_losses\n",
        "\n",
        "def evaluate(model, line, category_indices):\n",
        "    \"\"\"Evaluate the model on a single input\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        line_tensor = line_to_tensor(line)\n",
        "        hidden = model.init_hidden(batch_size=line_tensor.size(0))\n",
        "        output, hidden = model(line_tensor, hidden)\n",
        "\n",
        "        # Get top prediction\n",
        "        _, predicted = output.topk(1)\n",
        "        category_idx = predicted.item()\n",
        "        predicted_category = category_indices[category_idx]\n",
        "\n",
        "        # Get probability\n",
        "        prob = torch.exp(output[0][category_idx]).item()\n",
        "\n",
        "        return predicted_category, prob\n",
        "\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    hidden_size = 128\n",
        "    n_iters = 10000\n",
        "    learning_rate = 0.005\n",
        "\n",
        "    # Load data\n",
        "    binary_data = load_eng_fra_data('./data/eng-fra.txt')\n",
        "    names_data, categories = load_names_data('./data/names')\n",
        "\n",
        "    if not binary_data and not names_data:\n",
        "        print(\"No data loaded. Please check file paths.\")\n",
        "        return\n",
        "\n",
        "    # Binary classification\n",
        "    if binary_data:\n",
        "        print(\"\\nTraining binary classifier (English-French)...\")\n",
        "        binary_categories = ['English', 'French']\n",
        "        model_binary = RNNModel(n_characters, hidden_size, len(binary_categories))\n",
        "        binary_losses = train(model_binary, binary_data, binary_categories, n_iters, learning_rate)\n",
        "\n",
        "        # Test binary classifier\n",
        "        test_words = ['hello', 'bonjour', 'world', 'monde']\n",
        "        print(\"\\nTesting binary classifier:\")\n",
        "        for word in test_words:\n",
        "            pred_category, confidence = evaluate(model_binary, word, binary_categories)\n",
        "            print(f\"'{word}' -> {pred_category} ({confidence:.2%} confident)\")\n",
        "\n",
        "    # Multiclass classification\n",
        "    if names_data:\n",
        "        print(\"\\nTraining multiclass classifier (Names)...\")\n",
        "        model_multiclass = RNNModel(n_characters, hidden_size, len(categories))\n",
        "        multiclass_losses = train(model_multiclass, names_data, categories, n_iters, learning_rate)\n",
        "\n",
        "        # Test multiclass classifier\n",
        "        test_names = ['Mary', 'Giovanni', 'Chen', 'Satoshi']\n",
        "        print(\"\\nTesting multiclass classifier:\")\n",
        "        for name in test_names:\n",
        "            pred_category, confidence = evaluate(model_multiclass, name, categories)\n",
        "            print(f\"'{name}' -> {pred_category} ({confidence:.2%} confident)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILVblT5rgpV3",
        "outputId": "fa27fb83-946d-4dda-face-83082da543f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training binary classifier (English-French)...\n",
            "Iteration: 100/10000 | Loss: 0.9020 | Example: Peuvent-elles me voir ? -> French\n",
            "Iteration: 200/10000 | Loss: 0.9808 | Example: I am glad that you have succeeded. -> English\n",
            "Iteration: 300/10000 | Loss: 0.8548 | Example: The light has turned green. -> English\n",
            "Iteration: 400/10000 | Loss: 0.9540 | Example: Let him go. -> English\n",
            "Iteration: 500/10000 | Loss: 0.8260 | Example: A revolution broke out in Mexico. -> English\n",
            "Iteration: 600/10000 | Loss: 0.8481 | Example: My wife is cooking. -> English\n",
            "Iteration: 700/10000 | Loss: 0.8973 | Example: Je voulais simplement m'en assurer. -> French\n",
            "Iteration: 800/10000 | Loss: 0.9348 | Example: I'm stubborn. -> English\n",
            "Iteration: 900/10000 | Loss: 0.8466 | Example: Tom is the one that doesn't like me. -> English\n",
            "Iteration: 1000/10000 | Loss: 0.8746 | Example: La fermeture Éclair de ton pantalon est baissée. -> French\n",
            "Iteration: 1100/10000 | Loss: 0.9640 | Example: Il y a toujours une première fois pour tout. -> French\n",
            "Iteration: 1200/10000 | Loss: 0.8090 | Example: Elle m'a tout dit. -> French\n",
            "Iteration: 1300/10000 | Loss: 0.9355 | Example: We're going to do everything we can. -> English\n",
            "Iteration: 1400/10000 | Loss: 0.9216 | Example: Did you enjoy yourself at the party last night? -> English\n",
            "Iteration: 1500/10000 | Loss: 0.7788 | Example: Would you be willing to send me a sample free of charge? -> English\n",
            "Iteration: 1600/10000 | Loss: 0.8462 | Example: Elles ne peuvent pas toutes être mauvaises. -> French\n",
            "Iteration: 1700/10000 | Loss: 0.9492 | Example: She found him handsome. -> English\n",
            "Iteration: 1800/10000 | Loss: 0.9060 | Example: Who are you trying to convince? -> English\n",
            "Iteration: 1900/10000 | Loss: 0.7744 | Example: Nous sommes tous d'accord avec toi. -> French\n",
            "Iteration: 2000/10000 | Loss: 0.8894 | Example: Son oncle est mort il y a cinq ans. -> French\n",
            "Iteration: 2100/10000 | Loss: 0.8507 | Example: C'est un artiste célèbre. -> French\n",
            "Iteration: 2200/10000 | Loss: 0.9175 | Example: I'm bored. Let's do something. -> English\n",
            "Iteration: 2300/10000 | Loss: 0.8588 | Example: The conversation moved on to other topics. -> English\n",
            "Iteration: 2400/10000 | Loss: 0.9212 | Example: Tom, do you still love me? -> English\n",
            "Iteration: 2500/10000 | Loss: 0.8867 | Example: He is now in a very difficult situation. -> English\n",
            "Iteration: 2600/10000 | Loss: 0.7739 | Example: Le Japon possède beaucoup de belles montagnes. -> French\n",
            "Iteration: 2700/10000 | Loss: 0.8756 | Example: Je n'aime pas les détails. -> French\n",
            "Iteration: 2800/10000 | Loss: 0.8299 | Example: M'accusez-vous d'être un menteur ? -> French\n",
            "Iteration: 2900/10000 | Loss: 0.9312 | Example: En football américain un touché vaut six points. -> French\n",
            "Iteration: 3000/10000 | Loss: 0.8530 | Example: Cette chanson nous est familière. -> French\n",
            "Iteration: 3100/10000 | Loss: 1.0420 | Example: Elle se promena avant le petit-déjeuner. -> French\n",
            "Iteration: 3200/10000 | Loss: 0.8622 | Example: Ce qu'elles voulaient, c'était un homme comme lui. -> French\n",
            "Iteration: 3300/10000 | Loss: 0.8390 | Example: Je l'ai fait auparavant. -> French\n",
            "Iteration: 3400/10000 | Loss: 0.7940 | Example: Peut-être n'est-ce pas possible. -> French\n",
            "Iteration: 3500/10000 | Loss: 0.9563 | Example: That's a tough question to answer. -> English\n",
            "Iteration: 3600/10000 | Loss: 0.7988 | Example: Ça a été une bonne expérience. -> French\n",
            "Iteration: 3700/10000 | Loss: 0.9073 | Example: She panicked when she heard the news. -> English\n",
            "Iteration: 3800/10000 | Loss: 0.8325 | Example: It was superb. -> English\n",
            "Iteration: 3900/10000 | Loss: 0.7559 | Example: Ne me parle pas de religion ! -> French\n",
            "Iteration: 4000/10000 | Loss: 0.8322 | Example: Vous m'avez abandonné. -> French\n",
            "Iteration: 4100/10000 | Loss: 0.8212 | Example: I think that he's probably not a bad boy. -> English\n",
            "Iteration: 4200/10000 | Loss: 0.7377 | Example: Le déjeuner a une odeur délicieuse. -> French\n",
            "Iteration: 4300/10000 | Loss: 0.8432 | Example: J'ai vu une araignée qui se baladait sur le plafond. -> French\n",
            "Iteration: 4400/10000 | Loss: 0.7648 | Example: Je ne dispose d'aucune preuve. -> French\n",
            "Iteration: 4500/10000 | Loss: 0.8498 | Example: Remember to mail the letter on your way to school. -> English\n",
            "Iteration: 4600/10000 | Loss: 0.9773 | Example: J'ai fait taper la lettre à ma secrétaire. -> French\n",
            "Iteration: 4700/10000 | Loss: 0.9269 | Example: He seems to have left the country. -> English\n",
            "Iteration: 4800/10000 | Loss: 0.8524 | Example: I know you're not lying. -> English\n",
            "Iteration: 4900/10000 | Loss: 0.7843 | Example: D'accord, supposons que tu aies raison. -> French\n",
            "Iteration: 5000/10000 | Loss: 0.7983 | Example: Tom n'a pas accepté mes excuses. -> French\n",
            "Iteration: 5100/10000 | Loss: 0.8054 | Example: I don't know how I know. -> English\n",
            "Iteration: 5200/10000 | Loss: 0.8434 | Example: Tu n'es pas facile à trouver. -> French\n",
            "Iteration: 5300/10000 | Loss: 0.8655 | Example: The police are looking into it. -> English\n",
            "Iteration: 5400/10000 | Loss: 0.8636 | Example: I got you all right where I want you. -> English\n",
            "Iteration: 5500/10000 | Loss: 0.8152 | Example: Il est dommage que vous ne puissiez pas venir. -> French\n",
            "Iteration: 5600/10000 | Loss: 0.9432 | Example: A calamity was avoided by sheer luck. -> English\n",
            "Iteration: 5700/10000 | Loss: 0.7685 | Example: Merci de le rendre aussi facile. -> French\n",
            "Iteration: 5800/10000 | Loss: 0.9681 | Example: Je n'en ai pas la moindre idée. -> French\n",
            "Iteration: 5900/10000 | Loss: 0.8769 | Example: Life is like a game of chess. -> English\n",
            "Iteration: 6000/10000 | Loss: 0.7966 | Example: How come he didn't show up? -> English\n",
            "Iteration: 6100/10000 | Loss: 0.8906 | Example: Vous n'êtes pas très bon. -> French\n",
            "Iteration: 6200/10000 | Loss: 0.8872 | Example: Elle a les cheveux longs. -> French\n",
            "Iteration: 6300/10000 | Loss: 0.8218 | Example: I should have taken the money. -> English\n",
            "Iteration: 6400/10000 | Loss: 0.9165 | Example: Nous ne sommes pas apparentées. -> French\n",
            "Iteration: 6500/10000 | Loss: 0.8820 | Example: You don't have to be so sarcastic. -> English\n",
            "Iteration: 6600/10000 | Loss: 0.8617 | Example: This no longer matters. -> English\n",
            "Iteration: 6700/10000 | Loss: 0.8423 | Example: Je suis très triste d'entendre ça. -> French\n",
            "Iteration: 6800/10000 | Loss: 0.7765 | Example: Are you their mother? -> English\n",
            "Iteration: 6900/10000 | Loss: 0.9333 | Example: Penses-y. -> French\n",
            "Iteration: 7000/10000 | Loss: 0.8739 | Example: I don't want to get you into trouble. -> English\n",
            "Iteration: 7100/10000 | Loss: 1.0187 | Example: I want a full report before 2:30. -> English\n",
            "Iteration: 7200/10000 | Loss: 0.8490 | Example: Tom ne l'a jamais refait. -> French\n",
            "Iteration: 7300/10000 | Loss: 0.8665 | Example: I think we may have something that you'd be interested in buying. -> English\n",
            "Iteration: 7400/10000 | Loss: 0.8885 | Example: Je commençais à m'inquiéter à votre sujet. -> French\n",
            "Iteration: 7500/10000 | Loss: 0.9024 | Example: Rabbits have long ears. -> English\n",
            "Iteration: 7600/10000 | Loss: 0.8383 | Example: Vous pouvez rester avec nous pour l'instant. -> French\n",
            "Iteration: 7700/10000 | Loss: 0.7729 | Example: You're the girl of my dreams. -> English\n",
            "Iteration: 7800/10000 | Loss: 0.9303 | Example: Il porte un pyjama. -> French\n",
            "Iteration: 7900/10000 | Loss: 0.8181 | Example: Don't you tell me to relax. -> English\n",
            "Iteration: 8000/10000 | Loss: 0.9602 | Example: Tom a été tué dans un accident de voiture. -> French\n",
            "Iteration: 8100/10000 | Loss: 0.8233 | Example: J'ai peint le portail en bleu. -> French\n",
            "Iteration: 8200/10000 | Loss: 0.9036 | Example: He did not die of cancer. -> English\n",
            "Iteration: 8300/10000 | Loss: 0.8706 | Example: Come with us. -> English\n",
            "Iteration: 8400/10000 | Loss: 0.8545 | Example: Ce serait mieux d'essayer. -> French\n",
            "Iteration: 8500/10000 | Loss: 1.0403 | Example: Il faut juste que tu regardes ce que je fais. -> French\n",
            "Iteration: 8600/10000 | Loss: 1.0782 | Example: I want to find out what Tom did yesterday. -> English\n",
            "Iteration: 8700/10000 | Loss: 0.8223 | Example: Tom was badly beaten before being killed. -> English\n",
            "Iteration: 8800/10000 | Loss: 0.8779 | Example: C'est une niche que j'ai confectionnée moi-même. -> French\n",
            "Iteration: 8900/10000 | Loss: 0.8679 | Example: I was on the wrong track. -> English\n",
            "Iteration: 9000/10000 | Loss: 0.8741 | Example: I just don't want you to get upset. -> English\n",
            "Iteration: 9100/10000 | Loss: 0.8430 | Example: I can't believe I let you talk me into this. -> English\n",
            "Iteration: 9200/10000 | Loss: 0.8033 | Example: That's very thoughtful. -> English\n",
            "Iteration: 9300/10000 | Loss: 0.8290 | Example: Ça fait longtemps que je ne l'ai vu. -> French\n",
            "Iteration: 9400/10000 | Loss: 0.8573 | Example: You're resourceful. -> English\n",
            "Iteration: 9500/10000 | Loss: 0.9096 | Example: Nous l'avons fait. -> French\n",
            "Iteration: 9600/10000 | Loss: 0.9184 | Example: Some days seem to just drag on and last forever. -> English\n",
            "Iteration: 9700/10000 | Loss: 0.8743 | Example: Il serait contre-productif de faire une telle chose. -> French\n",
            "Iteration: 9800/10000 | Loss: 0.9576 | Example: Don't scare me like that! -> English\n",
            "Iteration: 9900/10000 | Loss: 0.8729 | Example: Certains pays en Europe ne font pas partie de l'Union Européenne. -> French\n",
            "Iteration: 10000/10000 | Loss: 0.8311 | Example: Je ne devrais pas boire. -> French\n",
            "\n",
            "Testing binary classifier:\n",
            "'hello' -> English (57.76% confident)\n",
            "'bonjour' -> English (83.47% confident)\n",
            "'world' -> English (57.76% confident)\n",
            "'monde' -> English (71.85% confident)\n",
            "\n",
            "Training multiclass classifier (Names)...\n",
            "Iteration: 100/10000 | Loss: 2.5626 | Example: Ablyazov -> Russian\n",
            "Iteration: 200/10000 | Loss: 2.2558 | Example: Meisner -> Dutch\n",
            "Iteration: 300/10000 | Loss: 2.2795 | Example: Schwangau -> German\n",
            "Iteration: 400/10000 | Loss: 2.2961 | Example: Foerstner -> German\n",
            "Iteration: 500/10000 | Loss: 2.0237 | Example: Underdown -> English\n",
            "Iteration: 600/10000 | Loss: 2.3552 | Example: Newington -> English\n",
            "Iteration: 700/10000 | Loss: 2.3100 | Example: Mochulov -> Russian\n",
            "Iteration: 800/10000 | Loss: 2.1611 | Example: Kober -> Czech\n",
            "Iteration: 900/10000 | Loss: 2.0319 | Example: Kloet -> Dutch\n",
            "Iteration: 1000/10000 | Loss: 2.4673 | Example: Auton -> English\n",
            "Iteration: 1100/10000 | Loss: 1.8304 | Example: Karubo -> Japanese\n",
            "Iteration: 1200/10000 | Loss: 2.3144 | Example: Lobo -> Portuguese\n",
            "Iteration: 1300/10000 | Loss: 1.9850 | Example: Nader -> Arabic\n",
            "Iteration: 1400/10000 | Loss: 2.2389 | Example: Tahan -> Arabic\n",
            "Iteration: 1500/10000 | Loss: 1.8403 | Example: Vaipan -> Russian\n",
            "Iteration: 1600/10000 | Loss: 2.3073 | Example: Silva -> Spanish\n",
            "Iteration: 1700/10000 | Loss: 2.1240 | Example: Adjubei -> Russian\n",
            "Iteration: 1800/10000 | Loss: 2.2662 | Example: Muzhdabaev -> Russian\n",
            "Iteration: 1900/10000 | Loss: 2.2948 | Example: Villa -> Italian\n",
            "Iteration: 2000/10000 | Loss: 2.3079 | Example: Gemmell -> English\n",
            "Iteration: 2100/10000 | Loss: 2.1141 | Example: Alfero -> Italian\n",
            "Iteration: 2200/10000 | Loss: 1.8820 | Example: Gashkov -> Russian\n",
            "Iteration: 2300/10000 | Loss: 2.3019 | Example: Navrkal -> Czech\n",
            "Iteration: 2400/10000 | Loss: 2.3067 | Example: Bruce -> English\n",
            "Iteration: 2500/10000 | Loss: 2.3571 | Example: Rijnder -> Dutch\n",
            "Iteration: 2600/10000 | Loss: 2.1455 | Example: Zhidenko -> Russian\n",
            "Iteration: 2700/10000 | Loss: 1.9432 | Example: Glazachev -> Russian\n",
            "Iteration: 2800/10000 | Loss: 2.0667 | Example: Muzhdabaev -> Russian\n",
            "Iteration: 2900/10000 | Loss: 2.0072 | Example: O'Bree -> Irish\n",
            "Iteration: 3000/10000 | Loss: 2.3773 | Example: Chang -> Korean\n",
            "Iteration: 3100/10000 | Loss: 2.1583 | Example: Vaisberg -> Russian\n",
            "Iteration: 3200/10000 | Loss: 2.1396 | Example: Romijnsen -> Dutch\n",
            "Iteration: 3300/10000 | Loss: 2.2444 | Example: Prokoshev -> Russian\n",
            "Iteration: 3400/10000 | Loss: 2.2218 | Example: Nutman -> English\n",
            "Iteration: 3500/10000 | Loss: 1.8056 | Example: Aswad -> Arabic\n",
            "Iteration: 3600/10000 | Loss: 2.1284 | Example: Sawyer -> English\n",
            "Iteration: 3700/10000 | Loss: 2.1708 | Example: Homutov -> Russian\n",
            "Iteration: 3800/10000 | Loss: 1.9884 | Example: Vadbolsky -> Russian\n",
            "Iteration: 3900/10000 | Loss: 2.2817 | Example: Groundon -> Russian\n",
            "Iteration: 4000/10000 | Loss: 1.8771 | Example: Kitchen -> English\n",
            "Iteration: 4100/10000 | Loss: 2.0594 | Example: Maynard -> English\n",
            "Iteration: 4200/10000 | Loss: 2.0847 | Example: Gudz -> Russian\n",
            "Iteration: 4300/10000 | Loss: 2.2159 | Example: Alferiev -> Russian\n",
            "Iteration: 4400/10000 | Loss: 2.0271 | Example: Kalaidjan -> Russian\n",
            "Iteration: 4500/10000 | Loss: 2.0023 | Example: Eretzky -> Russian\n",
            "Iteration: 4600/10000 | Loss: 2.3789 | Example: Dobrov -> Russian\n",
            "Iteration: 4700/10000 | Loss: 1.9628 | Example: Abukhoff -> Russian\n",
            "Iteration: 4800/10000 | Loss: 2.2321 | Example: Tommii -> Japanese\n",
            "Iteration: 4900/10000 | Loss: 1.8722 | Example: Palfreyman -> English\n",
            "Iteration: 5000/10000 | Loss: 2.2082 | Example: Nash -> English\n",
            "Iteration: 5100/10000 | Loss: 2.1873 | Example: Muzychka -> Russian\n",
            "Iteration: 5200/10000 | Loss: 2.2721 | Example: Kobayashi -> Japanese\n",
            "Iteration: 5300/10000 | Loss: 2.0558 | Example: Abboud -> Arabic\n",
            "Iteration: 5400/10000 | Loss: 2.0037 | Example: Piercey -> English\n",
            "Iteration: 5500/10000 | Loss: 2.0814 | Example: Vaginov -> Russian\n",
            "Iteration: 5600/10000 | Loss: 2.4612 | Example: Tchehovsky -> Russian\n",
            "Iteration: 5700/10000 | Loss: 2.1953 | Example: Guzeev -> Russian\n",
            "Iteration: 5800/10000 | Loss: 2.1023 | Example: Schmitz -> German\n",
            "Iteration: 5900/10000 | Loss: 2.2028 | Example: Takasugi -> Japanese\n",
            "Iteration: 6000/10000 | Loss: 2.0560 | Example: Glass -> English\n",
            "Iteration: 6100/10000 | Loss: 1.7503 | Example: Lynes -> English\n",
            "Iteration: 6200/10000 | Loss: 2.2025 | Example: Eglevsky -> Russian\n",
            "Iteration: 6300/10000 | Loss: 2.1225 | Example: Ohara -> English\n",
            "Iteration: 6400/10000 | Loss: 1.9936 | Example: Zhokhin -> Russian\n",
            "Iteration: 6500/10000 | Loss: 1.6727 | Example: Townend -> English\n",
            "Iteration: 6600/10000 | Loss: 2.1209 | Example: Gest -> Russian\n",
            "Iteration: 6700/10000 | Loss: 2.1372 | Example: Sayegh -> Arabic\n",
            "Iteration: 6800/10000 | Loss: 2.0721 | Example: Mihels -> Russian\n",
            "Iteration: 6900/10000 | Loss: 1.6373 | Example: Yakimovsky -> Russian\n",
            "Iteration: 7000/10000 | Loss: 2.0824 | Example: Shamoon -> Arabic\n",
            "Iteration: 7100/10000 | Loss: 1.8577 | Example: Bakuroff -> Russian\n",
            "Iteration: 7200/10000 | Loss: 2.2172 | Example: Lovell -> English\n",
            "Iteration: 7300/10000 | Loss: 1.9023 | Example: Jerebovich -> Russian\n",
            "Iteration: 7400/10000 | Loss: 2.1677 | Example: Ly -> Vietnamese\n",
            "Iteration: 7500/10000 | Loss: 1.8749 | Example: Pyrchenkov -> Russian\n",
            "Iteration: 7600/10000 | Loss: 2.0179 | Example: Mendel -> German\n",
            "Iteration: 7700/10000 | Loss: 2.1346 | Example: Rjeshotarsky -> Russian\n",
            "Iteration: 7800/10000 | Loss: 2.0715 | Example: Jagupets -> Russian\n",
            "Iteration: 7900/10000 | Loss: 2.2954 | Example: Dzheladze -> Russian\n",
            "Iteration: 8000/10000 | Loss: 1.8385 | Example: Yakuba -> Russian\n",
            "Iteration: 8100/10000 | Loss: 2.0877 | Example: Powis -> English\n",
            "Iteration: 8200/10000 | Loss: 1.8816 | Example: Turintsev -> Russian\n",
            "Iteration: 8300/10000 | Loss: 2.0223 | Example: Kate -> Japanese\n",
            "Iteration: 8400/10000 | Loss: 1.8299 | Example: Jacob -> English\n",
            "Iteration: 8500/10000 | Loss: 2.1908 | Example: Abulhanov -> Russian\n",
            "Iteration: 8600/10000 | Loss: 1.8294 | Example: Curtis -> English\n",
            "Iteration: 8700/10000 | Loss: 1.9812 | Example: Kriz -> Czech\n",
            "Iteration: 8800/10000 | Loss: 1.9011 | Example: Hakimi -> Arabic\n",
            "Iteration: 8900/10000 | Loss: 2.0494 | Example: Luckhurst -> English\n",
            "Iteration: 9000/10000 | Loss: 1.9906 | Example: Warner -> English\n",
            "Iteration: 9100/10000 | Loss: 1.9005 | Example: Zhigachev -> Russian\n",
            "Iteration: 9200/10000 | Loss: 2.5188 | Example: Babadzhanyan -> Russian\n",
            "Iteration: 9300/10000 | Loss: 1.7320 | Example: Huggins -> English\n",
            "Iteration: 9400/10000 | Loss: 1.9538 | Example: Arnott -> English\n",
            "Iteration: 9500/10000 | Loss: 2.0816 | Example: Zhulidov -> Russian\n",
            "Iteration: 9600/10000 | Loss: 1.9935 | Example: Pokhilchuk -> Russian\n",
            "Iteration: 9700/10000 | Loss: 1.7270 | Example: Kaberov -> Russian\n",
            "Iteration: 9800/10000 | Loss: 2.2353 | Example: Lathey -> English\n",
            "Iteration: 9900/10000 | Loss: 1.9110 | Example: Dovbyschuk -> Russian\n",
            "Iteration: 10000/10000 | Loss: 1.8280 | Example: Awetisoff -> Russian\n",
            "\n",
            "Testing multiclass classifier:\n",
            "'Mary' -> Russian (30.73% confident)\n",
            "'Giovanni' -> Dutch (28.26% confident)\n",
            "'Chen' -> Russian (35.01% confident)\n",
            "'Satoshi' -> Russian (58.95% confident)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import string\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "# Character set definition\n",
        "all_characters = string.ascii_letters + \" .,;'-!?()\"\n",
        "n_characters = len(all_characters)\n",
        "char_to_index = {ch: i for i, ch in enumerate(all_characters)}\n",
        "\n",
        "def line_to_tensor(line):\n",
        "    \"\"\"Convert string to tensor with correct dimensions\"\"\"\n",
        "    tensor = torch.zeros(1, len(line), n_characters)  # (batch, seq, features)\n",
        "    for i, char in enumerate(line):\n",
        "        if char in char_to_index:\n",
        "            tensor[0][i][char_to_index[char]] = 1\n",
        "    return tensor\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.2):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # x shape: (batch, seq, feature)\n",
        "        out, (hidden, cell) = self.lstm(x, hidden)\n",
        "        out = self.dropout(out[:, -1, :])  # Use last output\n",
        "        out = self.fc(out)\n",
        "        out = self.softmax(out)\n",
        "        return out, (hidden, cell)\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        # (num_layers, batch, hidden_size)\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
        "\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.2):\n",
        "        super(GRUClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # x shape: (batch, seq, feature)\n",
        "        out, hidden = self.gru(x, hidden)\n",
        "        out = self.dropout(out[:, -1, :])  # Use last output\n",
        "        out = self.fc(out)\n",
        "        out = self.softmax(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        # (num_layers, batch, hidden_size)\n",
        "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "\n",
        "def load_data(data_path, data_type='names'):\n",
        "    \"\"\"Load either names or eng-fra data\"\"\"\n",
        "    if data_type == 'names':\n",
        "        data = []\n",
        "        categories = []\n",
        "        for filename in os.listdir(data_path):\n",
        "            if filename.endswith(\".txt\"):\n",
        "                category = os.path.splitext(filename)[0]\n",
        "                categories.append(category)\n",
        "                with open(os.path.join(data_path, filename), 'r', encoding='utf-8') as f:\n",
        "                    names = f.read().strip().split('\\n')\n",
        "                    data.extend([(name, category) for name in names])\n",
        "        return data, categories\n",
        "    else:  # eng-fra\n",
        "        data = []\n",
        "        with open(data_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) == 2:\n",
        "                    data.append((parts[0], 'English'))\n",
        "                    data.append((parts[1], 'French'))\n",
        "        return data, ['English', 'French']\n",
        "\n",
        "def train_model(model_class, data, categories, model_params, train_params):\n",
        "    \"\"\"Train a model with given parameters\"\"\"\n",
        "    # Create model\n",
        "    model = model_class(**model_params)\n",
        "    model.train()\n",
        "\n",
        "    # Training setup\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=train_params['learning_rate'])\n",
        "\n",
        "    # Training loop\n",
        "    start_time = time.time()\n",
        "    current_loss = 0\n",
        "    all_losses = []\n",
        "\n",
        "    for iter in range(1, train_params['n_iterations'] + 1):\n",
        "        # Get random training example\n",
        "        line, category = random.choice(data)\n",
        "        category_tensor = torch.tensor([categories.index(category)], dtype=torch.long)\n",
        "        line_tensor = line_to_tensor(line)\n",
        "\n",
        "        # Training step\n",
        "        hidden = model.init_hidden()\n",
        "        model.zero_grad()\n",
        "\n",
        "        output, hidden = model(line_tensor, hidden)\n",
        "        loss = criterion(output, category_tensor)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        if iter % train_params['print_every'] == 0:\n",
        "            avg_loss = current_loss / train_params['print_every']\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f'Iteration: {iter}/{train_params[\"n_iterations\"]} | '\n",
        "                  f'Loss: {avg_loss:.4f} | Time: {elapsed:.1f}s | '\n",
        "                  f'Example: {line} -> {category}')\n",
        "            current_loss = 0\n",
        "            all_losses.append(avg_loss)\n",
        "\n",
        "    return model, all_losses\n",
        "\n",
        "def evaluate(model, line, categories):\n",
        "    \"\"\"Evaluate model on single input\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        line_tensor = line_to_tensor(line)\n",
        "        hidden = model.init_hidden()\n",
        "        output, _ = model(line_tensor, hidden)\n",
        "\n",
        "        # Get prediction\n",
        "        _, predicted = output.topk(1)\n",
        "        category_idx = predicted.item()\n",
        "        predicted_category = categories[category_idx]\n",
        "\n",
        "        # Get confidence\n",
        "        prob = torch.exp(output[0][category_idx]).item()\n",
        "\n",
        "        return predicted_category, prob\n",
        "\n",
        "def main():\n",
        "    # Model parameters\n",
        "    model_params = {\n",
        "        'input_size': n_characters,\n",
        "        'hidden_size': 256,\n",
        "        'output_size': None,  # Will be set based on data\n",
        "        'num_layers': 2,\n",
        "        'dropout': 0.2\n",
        "    }\n",
        "\n",
        "    # Training parameters\n",
        "    train_params = {\n",
        "        'n_iterations': 10000,\n",
        "        'learning_rate': 0.001,\n",
        "        'print_every': 500\n",
        "    }\n",
        "\n",
        "    # Load and train on both datasets with both models\n",
        "    for data_type in ['binary', 'multiclass']:\n",
        "        print(f\"\\nTraining on {data_type} classification task...\")\n",
        "\n",
        "        # Load appropriate data\n",
        "        if data_type == 'binary':\n",
        "            data, categories = load_data('./data/eng-fra.txt', 'eng-fra')\n",
        "        else:\n",
        "            data, categories = load_data('./data/names', 'names')\n",
        "\n",
        "        model_params['output_size'] = len(categories)\n",
        "\n",
        "        # Train LSTM\n",
        "        print(\"\\nTraining LSTM model...\")\n",
        "        lstm_model, lstm_losses = train_model(\n",
        "            LSTMClassifier, data, categories, model_params, train_params)\n",
        "\n",
        "        # Train GRU\n",
        "        print(\"\\nTraining GRU model...\")\n",
        "        gru_model, gru_losses = train_model(\n",
        "            GRUClassifier, data, categories, model_params, train_params)\n",
        "\n",
        "        # Test both models\n",
        "        test_words = ['hello', 'bonjour', 'world', 'monde'] if data_type == 'binary' else \\\n",
        "                     ['Mary', 'Giovanni', 'Chen', 'Satoshi']\n",
        "\n",
        "        print(\"\\nTesting models:\")\n",
        "        for word in test_words:\n",
        "            # Test LSTM\n",
        "            lstm_pred, lstm_conf = evaluate(lstm_model, word, categories)\n",
        "            # Test GRU\n",
        "            gru_pred, gru_conf = evaluate(gru_model, word, categories)\n",
        "\n",
        "            print(f\"\\nInput: '{word}'\")\n",
        "            print(f\"LSTM: {lstm_pred} ({lstm_conf:.2%} confident)\")\n",
        "            print(f\"GRU:  {gru_pred} ({gru_conf:.2%} confident)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aqhpn1pffQWV",
        "outputId": "b9153ec0-262e-40e6-972b-261c6666b848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on binary classification task...\n",
            "\n",
            "Training LSTM model...\n",
            "Iteration: 500/10000 | Loss: 0.7373 | Time: 9.8s | Example: Her father works at the bank. -> English\n",
            "Iteration: 1000/10000 | Loss: 0.7046 | Time: 19.8s | Example: He will wait. -> English\n",
            "Iteration: 1500/10000 | Loss: 0.7033 | Time: 30.2s | Example: Je ne peux rien faire. -> French\n",
            "Iteration: 2000/10000 | Loss: 0.6919 | Time: 40.4s | Example: Je pense que c'est bien pour toi que tu lises ce livre. -> French\n",
            "Iteration: 2500/10000 | Loss: 0.7000 | Time: 50.3s | Example: Ils se sont mis d'accord pour commencer tôt. -> French\n",
            "Iteration: 3000/10000 | Loss: 0.6975 | Time: 60.0s | Example: I am a tourist. -> English\n",
            "Iteration: 3500/10000 | Loss: 0.6980 | Time: 70.3s | Example: Combien de canettes de bière avez-vous bu ? -> French\n",
            "Iteration: 4000/10000 | Loss: 0.7021 | Time: 80.6s | Example: J'étais seul sans elle. -> French\n",
            "Iteration: 4500/10000 | Loss: 0.6982 | Time: 91.0s | Example: Il y a quelque chose que je dois te dire avant que tu ne partes. -> French\n",
            "Iteration: 5000/10000 | Loss: 0.6953 | Time: 100.5s | Example: Please wait half an hour. -> English\n",
            "Iteration: 5500/10000 | Loss: 0.7043 | Time: 111.1s | Example: I couldn't understand the announcement that was just made. -> English\n",
            "Iteration: 6000/10000 | Loss: 0.6966 | Time: 121.7s | Example: J'ai donné mon siège à la vieille dame. -> French\n",
            "Iteration: 6500/10000 | Loss: 0.6978 | Time: 132.2s | Example: Quiconque se trouve-t-il là-dedans ? -> French\n",
            "Iteration: 7000/10000 | Loss: 0.6961 | Time: 142.2s | Example: Avez-vous jamais été grosse ? -> French\n",
            "Iteration: 7500/10000 | Loss: 0.6973 | Time: 151.8s | Example: I really like you a lot. -> English\n",
            "Iteration: 8000/10000 | Loss: 0.6933 | Time: 162.2s | Example: I need air. -> English\n",
            "Iteration: 8500/10000 | Loss: 0.6965 | Time: 172.5s | Example: I have the house all to myself. -> English\n",
            "Iteration: 9000/10000 | Loss: 0.6921 | Time: 182.9s | Example: Have you been listening to me? -> English\n",
            "Iteration: 9500/10000 | Loss: 0.6970 | Time: 192.7s | Example: Tom est un agent des services correctionnels. -> French\n",
            "Iteration: 10000/10000 | Loss: 0.6941 | Time: 203.0s | Example: Dépêchez-vous, ou vous serez en retard pour le dernier train. -> French\n",
            "\n",
            "Training GRU model...\n",
            "Iteration: 500/10000 | Loss: 0.6522 | Time: 23.2s | Example: My sister married him in spite of our objections. -> English\n",
            "Iteration: 1000/10000 | Loss: 0.5217 | Time: 46.6s | Example: Pourquoi êtes-vous si dure avec vous-même ? -> French\n",
            "Iteration: 1500/10000 | Loss: 0.4040 | Time: 70.4s | Example: Il y avait de nombreuses choses qui requéraient mon attention, je ne suis donc rentré chez moi qu'après minuit. -> French\n",
            "Iteration: 2000/10000 | Loss: 0.3590 | Time: 92.0s | Example: You can't pick who you fall for. -> English\n",
            "Iteration: 2500/10000 | Loss: 0.2772 | Time: 114.7s | Example: A famous architect built this house. -> English\n",
            "Iteration: 3000/10000 | Loss: 0.3076 | Time: 137.5s | Example: Thank you for the other day. -> English\n",
            "Iteration: 3500/10000 | Loss: 0.2436 | Time: 160.5s | Example: Tom is reading in his bedroom. -> English\n",
            "Iteration: 4000/10000 | Loss: 0.2732 | Time: 183.2s | Example: Even though the weather was bad, I decided to go out. -> English\n",
            "Iteration: 4500/10000 | Loss: 0.2074 | Time: 206.1s | Example: I figured I'd find you here. -> English\n",
            "Iteration: 5000/10000 | Loss: 0.3587 | Time: 228.9s | Example: Les négociations sont terminées. -> French\n",
            "Iteration: 5500/10000 | Loss: 0.2907 | Time: 250.8s | Example: It is no good talking about it. -> English\n",
            "Iteration: 6000/10000 | Loss: 0.2436 | Time: 273.4s | Example: Tous les gens qui se rendent à l'église croient en Dieu. -> French\n",
            "Iteration: 6500/10000 | Loss: 0.2750 | Time: 296.9s | Example: The meeting was canceled because of the typhoon. -> English\n",
            "Iteration: 7000/10000 | Loss: 0.2734 | Time: 319.8s | Example: We're pooped. -> English\n",
            "Iteration: 7500/10000 | Loss: 0.2197 | Time: 342.4s | Example: Je veux te parler après le cours. -> French\n",
            "Iteration: 8000/10000 | Loss: 0.2161 | Time: 364.2s | Example: Ne le dites à quiconque ! -> French\n",
            "Iteration: 8500/10000 | Loss: 0.2981 | Time: 387.3s | Example: I have something to show you. -> English\n",
            "Iteration: 9000/10000 | Loss: 0.2498 | Time: 410.4s | Example: Je me suis fait nettoyer les chaussures. -> French\n",
            "Iteration: 9500/10000 | Loss: 0.1510 | Time: 432.9s | Example: Mange tes repas lentement. -> French\n",
            "Iteration: 10000/10000 | Loss: 0.1138 | Time: 454.9s | Example: It's a big country. -> English\n",
            "\n",
            "Testing models:\n",
            "\n",
            "Input: 'hello'\n",
            "LSTM: French (51.60% confident)\n",
            "GRU:  English (99.93% confident)\n",
            "\n",
            "Input: 'bonjour'\n",
            "LSTM: French (51.66% confident)\n",
            "GRU:  English (70.62% confident)\n",
            "\n",
            "Input: 'world'\n",
            "LSTM: French (51.61% confident)\n",
            "GRU:  English (99.11% confident)\n",
            "\n",
            "Input: 'monde'\n",
            "LSTM: French (51.53% confident)\n",
            "GRU:  French (76.76% confident)\n",
            "\n",
            "Training on multiclass classification task...\n",
            "\n",
            "Training LSTM model...\n",
            "Iteration: 500/10000 | Loss: 1.9237 | Time: 6.8s | Example: Neri -> Italian\n",
            "Iteration: 1000/10000 | Loss: 1.6002 | Time: 12.9s | Example: Jirinovsky -> Russian\n",
            "Iteration: 1500/10000 | Loss: 1.5088 | Time: 19.9s | Example: Rotaru -> Russian\n",
            "Iteration: 2000/10000 | Loss: 1.4766 | Time: 26.1s | Example: Asylmuratov -> Russian\n",
            "Iteration: 2500/10000 | Loss: 1.3047 | Time: 33.1s | Example: Koury -> Arabic\n",
            "Iteration: 3000/10000 | Loss: 1.3761 | Time: 39.3s | Example: John -> English\n",
            "Iteration: 3500/10000 | Loss: 1.3485 | Time: 46.3s | Example: Mochan -> Irish\n",
            "Iteration: 4000/10000 | Loss: 1.0878 | Time: 52.6s | Example: Antar -> Arabic\n",
            "Iteration: 4500/10000 | Loss: 1.2086 | Time: 59.6s | Example: Koumans -> Dutch\n",
            "Iteration: 5000/10000 | Loss: 1.2044 | Time: 65.8s | Example: Mustafa -> Arabic\n",
            "Iteration: 5500/10000 | Loss: 1.1314 | Time: 72.9s | Example: Como -> Italian\n",
            "Iteration: 6000/10000 | Loss: 1.1124 | Time: 79.0s | Example: Yagich -> Russian\n",
            "Iteration: 6500/10000 | Loss: 1.0872 | Time: 86.3s | Example: Botros -> Arabic\n",
            "Iteration: 7000/10000 | Loss: 1.0449 | Time: 92.6s | Example: Rogerson -> English\n",
            "Iteration: 7500/10000 | Loss: 1.0851 | Time: 99.6s | Example: Jbanov -> Russian\n",
            "Iteration: 8000/10000 | Loss: 0.9918 | Time: 106.0s | Example: Vitov -> Russian\n",
            "Iteration: 8500/10000 | Loss: 1.0454 | Time: 112.9s | Example: Ishimura -> Japanese\n",
            "Iteration: 9000/10000 | Loss: 0.8727 | Time: 119.4s | Example: Morioka -> Japanese\n",
            "Iteration: 9500/10000 | Loss: 0.9895 | Time: 126.2s | Example: Schuchard -> German\n",
            "Iteration: 10000/10000 | Loss: 1.0324 | Time: 132.5s | Example: Vasyukov -> Russian\n",
            "\n",
            "Training GRU model...\n",
            "Iteration: 500/10000 | Loss: 1.8121 | Time: 7.8s | Example: Zolotavin -> Russian\n",
            "Iteration: 1000/10000 | Loss: 1.4520 | Time: 15.5s | Example: Maroun -> Arabic\n",
            "Iteration: 1500/10000 | Loss: 1.2742 | Time: 22.2s | Example: Kilshaw -> English\n",
            "Iteration: 2000/10000 | Loss: 1.2808 | Time: 29.8s | Example: Gell -> English\n",
            "Iteration: 2500/10000 | Loss: 1.0902 | Time: 36.9s | Example: Borgogni -> Italian\n",
            "Iteration: 3000/10000 | Loss: 1.2109 | Time: 44.7s | Example: Lapir -> Russian\n",
            "Iteration: 3500/10000 | Loss: 1.0753 | Time: 52.1s | Example: Greenshields -> English\n",
            "Iteration: 4000/10000 | Loss: 1.0748 | Time: 59.7s | Example: Issa -> Arabic\n",
            "Iteration: 4500/10000 | Loss: 0.9704 | Time: 67.5s | Example: Vadbolsky -> Russian\n",
            "Iteration: 5000/10000 | Loss: 1.0527 | Time: 74.5s | Example: Arian -> Arabic\n",
            "Iteration: 5500/10000 | Loss: 0.9531 | Time: 82.2s | Example: Maksimchik -> Russian\n",
            "Iteration: 6000/10000 | Loss: 0.9119 | Time: 89.1s | Example: Mccabe -> English\n",
            "Iteration: 6500/10000 | Loss: 1.0113 | Time: 96.7s | Example: Elliston -> English\n",
            "Iteration: 7000/10000 | Loss: 0.9159 | Time: 103.9s | Example: Leggett -> English\n",
            "Iteration: 7500/10000 | Loss: 0.9764 | Time: 111.4s | Example: Daugelo -> Russian\n",
            "Iteration: 8000/10000 | Loss: 0.8566 | Time: 119.4s | Example: Hintzen -> German\n",
            "Iteration: 8500/10000 | Loss: 0.8223 | Time: 126.4s | Example: Dzhemal -> Russian\n",
            "Iteration: 9000/10000 | Loss: 0.8899 | Time: 134.1s | Example: Zavolokin -> Russian\n",
            "Iteration: 9500/10000 | Loss: 0.9357 | Time: 141.0s | Example: Peary -> Czech\n",
            "Iteration: 10000/10000 | Loss: 0.8422 | Time: 149.2s | Example: Sitta -> Czech\n",
            "\n",
            "Testing models:\n",
            "\n",
            "Input: 'Mary'\n",
            "LSTM: Arabic (54.68% confident)\n",
            "GRU:  English (39.41% confident)\n",
            "\n",
            "Input: 'Giovanni'\n",
            "LSTM: Italian (65.88% confident)\n",
            "GRU:  Italian (86.56% confident)\n",
            "\n",
            "Input: 'Chen'\n",
            "LSTM: English (35.33% confident)\n",
            "GRU:  Chinese (51.04% confident)\n",
            "\n",
            "Input: 'Satoshi'\n",
            "LSTM: Arabic (80.36% confident)\n",
            "GRU:  Japanese (64.19% confident)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "INoFek3ofQYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dyPZhFkkfQbQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}