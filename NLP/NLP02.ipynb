{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Perform all the preprocessing task after reading the text file.\n",
        "\n",
        "Text Cleaning :\n",
        "Convert text to lowercase.\n",
        "\n",
        "Remove special characters and punctuation.\n",
        "Remove extra white spaces.\n",
        "\n",
        "\n",
        "Tokenization :\n",
        "Split text into individual words or tokens.\n",
        "\n",
        "Stop Words Removal:\n",
        "Identify and remove common stop words (e.g., \"and\", \"the\", \"is\").\n",
        "Create a custom list based on the context.\n",
        "\n",
        "Stemming:\n",
        "Reduce words to their root form (e.g., \"running\" to \"run\").\n",
        "Use stemming algorithms like Porter or Snowball.\n",
        "\n",
        "Normalization\n",
        "\n",
        "Expand contractions (e.g., \"don't\" to \"do not\").\n",
        "Eliminate URLs, email addresses, and other alphabetic elements (special characters).\n",
        "Remove or handle numerical values if not relevant."
      ],
      "metadata": {
        "id": "ghpiKvgkHakY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "lFPVcUXe9rDq",
        "outputId": "84eb0c55-7e1b-4704-8dad-316583164cd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your input file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bac5d21c-1eab-45a6-8b57-88abba9b0d5c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bac5d21c-1eab-45a6-8b57-88abba9b0d5c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Assignment 2.txt to Assignment 2 (4).txt\n",
            "Original Text:\n",
            "\"Artificial Intelligence (AI) has become a cornerstone of modern technology, influencing various industries from healthcare to finance. With advancements in machine learning, AI systems can now analyze massive datasets to uncover hidden patterns and provide insights that drive decision making. It don't need human engagement For example, AI powered algorithms @user1 are frequently used in financial markets to predict stock prices and in healthcare to ~! $$$ assist in diagnosing diseases with 45 high accuracy. However, as AI technology continues to evolve, it doesn't raises important @user2 questions about data privacy, the potential for job displacement, and the ethical implications of automating complex processes. In addition to these concerns, AI's impact on daily life is profound, with applications ranging from personalized recommendations on streaming services to smart home devices that make our 95 lives more convenient. The discussion around AI also includes the #change challenge of ensuring fairness and avoiding biases that could adversely affect marginalized communities.\"\n",
            "\n",
            "Processing Steps:\n",
            "\n",
            "1. Converting to lowercase:\n",
            "\"artificial intelligence (ai) has become a cornerstone of modern technology, influencing various industries from healthcare to finance. with advancements in machine learning, ai systems can now analyze massive datasets to uncover hidden patterns and provide insights that drive decision making. it don't need human engagement for example, ai powered algorithms @user1 are frequently used in financial markets to predict stock prices and in healthcare to ~! $$$ assist in diagnosing diseases with 45 high accuracy. however, as ai technology continues to evolve, it doesn't raises important @user2 questions about data privacy, the potential for job displacement, and the ethical implications of automating complex processes. in addition to these concerns, ai's impact on daily life is profound, with applications ranging from personalized recommendations on streaming services to smart home devices that make our 95 lives more convenient. the discussion around ai also includes the #change challenge of ensuring fairness and avoiding biases that could adversely affect marginalized communities.\"\n",
            "\n",
            "8. Removing URLs and emails:\n",
            "\"artificial intelligence (ai) has become a cornerstone of modern technology, influencing various industries from healthcare to finance. with advancements in machine learning, ai systems can now analyze massive datasets to uncover hidden patterns and provide insights that drive decision making. it don't need human engagement for example, ai powered algorithms  are frequently used in financial markets to predict stock prices and in healthcare to ~! $$$ assist in diagnosing diseases with 45 high accuracy. however, as ai technology continues to evolve, it doesn't raises important  questions about data privacy, the potential for job displacement, and the ethical implications of automating complex processes. in addition to these concerns, ai's impact on daily life is profound, with applications ranging from personalized recommendations on streaming services to smart home devices that make our 95 lives more convenient. the discussion around ai also includes the #change challenge of ensuring fairness and avoiding biases that could adversely affect marginalized communities.\"\n",
            "\n",
            "7. Handling contractions:\n",
            "\"artificial intelligence (ai) has become a cornerstone of modern technology, influencing various industries from healthcare to finance. with advancements in machine learning, ai systems can now analyze massive datasets to uncover hidden patterns and provide insights that drive decision making. it do not need human engagement for example, ai powered algorithms  are frequently used in financial markets to predict stock prices and in healthcare to ~! $$$ assist in diagnosing diseases with 45 high accuracy. however, as ai technology continues to evolve, it does not raises important  questions about data privacy, the potential for job displacement, and the ethical implications of automating complex processes. in addition to these concerns, ai's impact on daily life is profound, with applications ranging from personalized recommendations on streaming services to smart home devices that make our 95 lives more convenient. the discussion around ai also includes the #change challenge of ensuring fairness and avoiding biases that could adversely affect marginalized communities.\"\n",
            "\n",
            "2. Removing special characters and punctuation:\n",
            " artificial intelligence  ai  has become a cornerstone of modern technology  influencing various industries from healthcare to finance  with advancements in machine learning  ai systems can now analyze massive datasets to uncover hidden patterns and provide insights that drive decision making  it do not need human engagement for example  ai powered algorithms  are frequently used in financial markets to predict stock prices and in healthcare to        assist in diagnosing diseases with    high accuracy  however  as ai technology continues to evolve  it does not raises important  questions about data privacy  the potential for job displacement  and the ethical implications of automating complex processes  in addition to these concerns  ai's impact on daily life is profound  with applications ranging from personalized recommendations on streaming services to smart home devices that make our    lives more convenient  the discussion around ai also includes the  change challenge of ensuring fairness and avoiding biases that could adversely affect marginalized communities  \n",
            "\n",
            "9. Removing numbers:\n",
            " artificial intelligence  ai  has become a cornerstone of modern technology  influencing various industries from healthcare to finance  with advancements in machine learning  ai systems can now analyze massive datasets to uncover hidden patterns and provide insights that drive decision making  it do not need human engagement for example  ai powered algorithms  are frequently used in financial markets to predict stock prices and in healthcare to        assist in diagnosing diseases with    high accuracy  however  as ai technology continues to evolve  it does not raises important  questions about data privacy  the potential for job displacement  and the ethical implications of automating complex processes  in addition to these concerns  ai's impact on daily life is profound  with applications ranging from personalized recommendations on streaming services to smart home devices that make our    lives more convenient  the discussion around ai also includes the  change challenge of ensuring fairness and avoiding biases that could adversely affect marginalized communities  \n",
            "\n",
            "3. Removing extra whitespace:\n",
            "artificial intelligence ai has become a cornerstone of modern technology influencing various industries from healthcare to finance with advancements in machine learning ai systems can now analyze massive datasets to uncover hidden patterns and provide insights that drive decision making it do not need human engagement for example ai powered algorithms are frequently used in financial markets to predict stock prices and in healthcare to assist in diagnosing diseases with high accuracy however as ai technology continues to evolve it does not raises important questions about data privacy the potential for job displacement and the ethical implications of automating complex processes in addition to these concerns ai's impact on daily life is profound with applications ranging from personalized recommendations on streaming services to smart home devices that make our lives more convenient the discussion around ai also includes the change challenge of ensuring fairness and avoiding biases that could adversely affect marginalized communities\n",
            "\n",
            "4. Tokenizing text:\n",
            "['artificial', 'intelligence', 'ai', 'has', 'become', 'a', 'cornerstone', 'of', 'modern', 'technology', 'influencing', 'various', 'industries', 'from', 'healthcare', 'to', 'finance', 'with', 'advancements', 'in', 'machine', 'learning', 'ai', 'systems', 'can', 'now', 'analyze', 'massive', 'datasets', 'to', 'uncover', 'hidden', 'patterns', 'and', 'provide', 'insights', 'that', 'drive', 'decision', 'making', 'it', 'do', 'not', 'need', 'human', 'engagement', 'for', 'example', 'ai', 'powered', 'algorithms', 'are', 'frequently', 'used', 'in', 'financial', 'markets', 'to', 'predict', 'stock', 'prices', 'and', 'in', 'healthcare', 'to', 'assist', 'in', 'diagnosing', 'diseases', 'with', 'high', 'accuracy', 'however', 'as', 'ai', 'technology', 'continues', 'to', 'evolve', 'it', 'does', 'not', 'raises', 'important', 'questions', 'about', 'data', 'privacy', 'the', 'potential', 'for', 'job', 'displacement', 'and', 'the', 'ethical', 'implications', 'of', 'automating', 'complex', 'processes', 'in', 'addition', 'to', 'these', 'concerns', 'ai', \"'s\", 'impact', 'on', 'daily', 'life', 'is', 'profound', 'with', 'applications', 'ranging', 'from', 'personalized', 'recommendations', 'on', 'streaming', 'services', 'to', 'smart', 'home', 'devices', 'that', 'make', 'our', 'lives', 'more', 'convenient', 'the', 'discussion', 'around', 'ai', 'also', 'includes', 'the', 'change', 'challenge', 'of', 'ensuring', 'fairness', 'and', 'avoiding', 'biases', 'that', 'could', 'adversely', 'affect', 'marginalized', 'communities']\n",
            "\n",
            "5. Removing stop words:\n",
            "['become', 'cornerstone', 'modern', 'technology', 'influencing', 'various', 'industries', 'healthcare', 'finance', 'advancements', 'machine', 'learning', 'systems', 'analyze', 'massive', 'datasets', 'uncover', 'hidden', 'patterns', 'provide', 'insights', 'drive', 'decision', 'making', 'need', 'human', 'engagement', 'example', 'powered', 'algorithms', 'frequently', 'used', 'financial', 'markets', 'predict', 'stock', 'prices', 'healthcare', 'assist', 'diagnosing', 'diseases', 'high', 'accuracy', 'however', 'technology', 'continues', 'evolve', 'raises', 'important', 'questions', 'data', 'privacy', 'potential', 'job', 'displacement', 'ethical', 'implications', 'automating', 'complex', 'processes', 'addition', 'concerns', \"'s\", 'impact', 'daily', 'life', 'profound', 'applications', 'ranging', 'personalized', 'recommendations', 'streaming', 'services', 'smart', 'home', 'devices', 'make', 'lives', 'convenient', 'discussion', 'around', 'also', 'includes', 'change', 'challenge', 'ensuring', 'fairness', 'avoiding', 'biases', 'could', 'adversely', 'affect', 'marginalized', 'communities']\n",
            "\n",
            "6. Applying stemming:\n",
            "['becom', 'cornerston', 'modern', 'technolog', 'influenc', 'variou', 'industri', 'healthcar', 'financ', 'advanc', 'machin', 'learn', 'system', 'analyz', 'massiv', 'dataset', 'uncov', 'hidden', 'pattern', 'provid', 'insight', 'drive', 'decis', 'make', 'need', 'human', 'engag', 'exampl', 'power', 'algorithm', 'frequent', 'use', 'financi', 'market', 'predict', 'stock', 'price', 'healthcar', 'assist', 'diagnos', 'diseas', 'high', 'accuraci', 'howev', 'technolog', 'continu', 'evolv', 'rais', 'import', 'question', 'data', 'privaci', 'potenti', 'job', 'displac', 'ethic', 'implic', 'autom', 'complex', 'process', 'addit', 'concern', \"'s\", 'impact', 'daili', 'life', 'profound', 'applic', 'rang', 'person', 'recommend', 'stream', 'servic', 'smart', 'home', 'devic', 'make', 'live', 'conveni', 'discuss', 'around', 'also', 'includ', 'chang', 'challeng', 'ensur', 'fair', 'avoid', 'bias', 'could', 'advers', 'affect', 'margin', 'commun']\n",
            "\n",
            "Final Processed Text:\n",
            "becom cornerston modern technolog influenc variou industri healthcar financ advanc machin learn system analyz massiv dataset uncov hidden pattern provid insight drive decis make need human engag exampl power algorithm frequent use financi market predict stock price healthcar assist diagnos diseas high accuraci howev technolog continu evolv rais import question data privaci potenti job displac ethic implic autom complex process addit concern 's impact daili life profound applic rang person recommend stream servic smart home devic make live conveni discuss around also includ chang challeng ensur fair avoid bias could advers affect margin commun\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#import contractions\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def read_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            return file.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        # Custom stop words (can be modified based on context)\n",
        "        self.custom_stop_words = set(['ai', 'artificial', 'intelligence'])\n",
        "        self.stop_words = set(stopwords.words('english')).union(self.custom_stop_words)\n",
        "\n",
        "    def convert_lowercase(self, text):\n",
        "        print(\"\\n1. Converting to lowercase:\")\n",
        "        result = text.lower()\n",
        "        print(result)\n",
        "        return result\n",
        "\n",
        "    def remove_special_chars(self, text):\n",
        "        print(\"\\n2. Removing special characters and punctuation:\")\n",
        "        # Keep apostrophes for contractions handling\n",
        "        result = re.sub(r'[^a-zA-Z\\s\\']', ' ', text)\n",
        "        print(result)\n",
        "        return result\n",
        "\n",
        "    def remove_extra_whitespace(self, text):\n",
        "        print(\"\\n3. Removing extra whitespace:\")\n",
        "        result = ' '.join(text.split())\n",
        "        print(result)\n",
        "        return result\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        print(\"\\n4. Tokenizing text:\")\n",
        "        tokens = word_tokenize(text)\n",
        "        print(tokens)\n",
        "        return tokens\n",
        "\n",
        "    def remove_stop_words(self, tokens):\n",
        "        print(\"\\n5. Removing stop words:\")\n",
        "        filtered_tokens = [word for word in tokens if word.lower() not in self.stop_words]\n",
        "        print(filtered_tokens)\n",
        "        return filtered_tokens\n",
        "\n",
        "    def apply_stemming(self, tokens):\n",
        "        print(\"\\n6. Applying stemming:\")\n",
        "        stemmed_tokens = [self.stemmer.stem(word) for word in tokens]\n",
        "        print(stemmed_tokens)\n",
        "        return stemmed_tokens\n",
        "\n",
        "    # def expand_contractions(self, text):\n",
        "    #     print(\"\\n7. Expanding contractions:\")\n",
        "    #     result = contractions.fix(text)\n",
        "    #     print(result)\n",
        "    #     return result\n",
        "    def expand_contractions(self, text):\n",
        "        print(\"\\n7. Handling contractions:\")\n",
        "        try:\n",
        "            result = contractions.fix(text)\n",
        "        except:\n",
        "            # Fallback method if contractions package fails\n",
        "            print(\"Using fallback contraction handling...\")\n",
        "            contractions_dict = {\n",
        "                \"don't\": \"do not\",\n",
        "                \"doesn't\": \"does not\",\n",
        "                \"won't\": \"will not\",\n",
        "                \"can't\": \"cannot\",\n",
        "                \"isn't\": \"is not\",\n",
        "                \"it's\": \"it is\",\n",
        "                \"I'm\": \"I am\",\n",
        "                \"aren't\": \"are not\"\n",
        "            }\n",
        "            result = text\n",
        "            for contraction, expansion in contractions_dict.items():\n",
        "                result = result.replace(contraction.lower(), expansion)\n",
        "        print(result)\n",
        "        return result\n",
        "\n",
        "\n",
        "    def remove_urls_emails(self, text):\n",
        "        print(\"\\n8. Removing URLs and emails:\")\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "        # Remove email addresses\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "        # Remove usernames (e.g., @user1)\n",
        "        text = re.sub(r'@\\w+', '', text)\n",
        "        print(text)\n",
        "        return text\n",
        "\n",
        "    def remove_numbers(self, text):\n",
        "        print(\"\\n9. Removing numbers:\")\n",
        "        result = re.sub(r'\\d+', '', text)\n",
        "        print(result)\n",
        "        return result\n",
        "\n",
        "    def process_text(self, text):\n",
        "        print(\"Original Text:\")\n",
        "        print(text)\n",
        "        print(\"\\nProcessing Steps:\")\n",
        "\n",
        "        # Step 1-3: Basic cleaning\n",
        "        text = self.convert_lowercase(text)\n",
        "        text = self.remove_urls_emails(text)\n",
        "        text = self.expand_contractions(text)\n",
        "        text = self.remove_special_chars(text)\n",
        "        text = self.remove_numbers(text)\n",
        "        text = self.remove_extra_whitespace(text)\n",
        "\n",
        "        # Step 4: Tokenization\n",
        "        tokens = self.tokenize_text(text)\n",
        "\n",
        "        # Step 5: Stop words removal\n",
        "        filtered_tokens = self.remove_stop_words(tokens)\n",
        "\n",
        "        # Step 6: Stemming\n",
        "        stemmed_tokens = self.apply_stemming(filtered_tokens)\n",
        "\n",
        "        print(\"\\nFinal Processed Text:\")\n",
        "        final_text = ' '.join(stemmed_tokens)\n",
        "        print(final_text)\n",
        "        return final_text\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    # For Google Colab, you might want to upload the file first\n",
        "    from google.colab import files\n",
        "    print(\"Please upload your input file...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Get the filename of the uploaded file\n",
        "    filename = next(iter(uploaded))\n",
        "\n",
        "    # Read the content of the uploaded file\n",
        "    text = read_file(filename)\n",
        "\n",
        "    if text:\n",
        "        # Initialize preprocessor\n",
        "        preprocessor = TextPreprocessor()\n",
        "\n",
        "        # Process the text\n",
        "        processed_text = preprocessor.process_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "anV4ubhiJkG4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}