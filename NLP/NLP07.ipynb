{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Priyanshu Raj"
      ],
      "metadata": {
        "id": "AarJn5uEg8hZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Use the Assignment 6 dataset for implementation).\n",
        "\n",
        "\n",
        "Design a decision tree and logistic regression model from scratch (without predefined function) to predict the likelihood of individuals defaulting on a loan based on various personal and financial features. The dataset contains information about 100 individuals, including their age, income, credit score, education level, and marital status, along with a binary target variable indicating whether they defaulted on a loan.\n",
        "\n",
        "The required task to perform :\n",
        "\n",
        "Data Understanding: Analyze the dataset to understand the relationships between the features and the target variable (loan default).\n",
        "\n",
        "Data Preprocessing: Clean the dataset and handle categorical variables using one-hot encoding.\n",
        "Normalize or standardize numerical features if necessary.\n",
        "\n",
        "Model Development:\n",
        "\n",
        "1. Implement a logistic regression and decision tree algorithm from scratch without using any predefined functions or libraries for logistic regression and decision tree.\n",
        "\n",
        "Split the data into training and testing in the ratio 80:20, and train the model on the training dataset.\n",
        "\n",
        "Model Evaluation:\n",
        "\n",
        "1. Evaluate the model’s performance on a remaining test dataset by calculating accuracy.\n",
        "2. Compare the model's accuracy between logistics regression and decision tree model designed with scratch.\n",
        "3. Summarize your observation in a paragraph/ bullet point.\n"
      ],
      "metadata": {
        "id": "dzEvheuPn64j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "TCjo8ao6hB6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = pd.read_csv('sample_credit_data.csv')"
      ],
      "metadata": {
        "id": "OLAo6QJ4hW5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "# One-hot encode categorical variables\n",
        "data_encoded = pd.get_dummies(data, columns=['Education_Level', 'Marital_Status'])"
      ],
      "metadata": {
        "id": "GnWd9SH3hXEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target variable\n",
        "X = data_encoded.drop(['Default'], axis=1)\n",
        "y = data_encoded['Default']"
      ],
      "metadata": {
        "id": "fEr_5O_Xl7he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "HnRDzjAghXGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Wg8zRemGhXJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression from scratch\n",
        "class LogisticRegressionScratch:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.num_iterations):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_predicted = self.sigmoid(linear_model)\n",
        "\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self.sigmoid(linear_model)\n",
        "        return [1 if i > 0.5 else 0 for i in y_predicted]"
      ],
      "metadata": {
        "id": "pUtNZUZHhXNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree from scratch\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=5):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1]\n",
        "        self.tree = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        if (depth >= self.max_depth or n_labels == 1 or n_samples < 2):\n",
        "            leaf_value = np.argmax(np.bincount(y))\n",
        "            return {'leaf_value': leaf_value}\n",
        "\n",
        "        feature_idx, threshold = self._best_split(X, y)\n",
        "        left_idxs = X[:, feature_idx] < threshold\n",
        "        right_idxs = ~left_idxs\n",
        "\n",
        "        left = self._grow_tree(X[left_idxs], y[left_idxs], depth+1)\n",
        "        right = self._grow_tree(X[right_idxs], y[right_idxs], depth+1)\n",
        "        return {'feature_idx': feature_idx, 'threshold': threshold, 'left': left, 'right': right}\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        m = X.shape[0]\n",
        "        if m <= 1:\n",
        "            return None, None\n",
        "\n",
        "        num_parent = [np.sum(y == c) for c in range(2)]\n",
        "        best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)\n",
        "        best_idx, best_thr = None, None\n",
        "\n",
        "        for idx in range(self.n_features):\n",
        "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
        "            num_left = [0, 0]\n",
        "            num_right = num_parent.copy()\n",
        "            for i in range(1, m):\n",
        "                c = classes[i - 1]\n",
        "                num_left[c] += 1\n",
        "                num_right[c] -= 1\n",
        "                gini_left = 1.0 - sum((num_left[x] / i) ** 2 for x in range(2))\n",
        "                gini_right = 1.0 - sum((num_right[x] / (m - i)) ** 2 for x in range(2))\n",
        "                gini = (i * gini_left + (m - i) * gini_right) / m\n",
        "                if thresholds[i] == thresholds[i - 1]:\n",
        "                    continue\n",
        "                if gini < best_gini:\n",
        "                    best_gini = gini\n",
        "                    best_idx = idx\n",
        "                    best_thr = (thresholds[i] + thresholds[i - 1]) / 2\n",
        "\n",
        "        return best_idx, best_thr\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if 'leaf_value' in node:\n",
        "            return node['leaf_value']\n",
        "\n",
        "        if x[node['feature_idx']] < node['threshold']:\n",
        "            return self._traverse_tree(x, node['left'])\n",
        "        return self._traverse_tree(x, node['right'])"
      ],
      "metadata": {
        "id": "hL456TJYhXP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "1YlV1UzAmj4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate the scratch model\n",
        "model_scratch = LogisticRegressionScratch()\n",
        "model_scratch.fit(X_train, y_train)\n",
        "y_pred_scratch = model_scratch.predict(X_test)\n",
        "lr_accuracy = accuracy_score(y_test, y_pred_scratch)\n",
        "print(f\"Accuracy of scratch model: {lr_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2af8MczHhXSY",
        "outputId": "3b038a5a-21a8-4fb1-cc65-6d98dbfc3f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of scratch model: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate Decision Tree\n",
        "dt_model = DecisionTree()\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "dt_accuracy = np.mean(dt_predictions == y_test)\n",
        "\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
        "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "yUC8BbiQhXUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8178298a-b239-482d-a825-3aedbc87212c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.9500\n",
            "Decision Tree Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze feature importance for Logistic Regression\n",
        "feature_importance_lr = pd.DataFrame({'Feature': X.columns, 'Importance': np.abs(model_scratch.weights)})\n",
        "feature_importance_lr = feature_importance_lr.sort_values('Importance', ascending=False)\n",
        "print(\"\\nTop 5 Important Features (Logistic Regression):\")\n",
        "print(feature_importance_lr.head())"
      ],
      "metadata": {
        "id": "cIoa352HhXXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01b9d252-09cb-40a7-c996-cfdf2b5f1a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 Important Features (Logistic Regression):\n",
            "                      Feature  Importance\n",
            "2                Credit_Score    1.569653\n",
            "1                      Income    0.520615\n",
            "0                         Age    0.260329\n",
            "5    Education_Level_Master's    0.150234\n",
            "3  Education_Level_Bachelor's    0.097498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze feature importance for Logistic Regression\n",
        "feature_importance_lr = pd.DataFrame({'Feature': X.columns, 'Importance': np.abs(model_scratch.weights)})\n",
        "feature_importance_lr = feature_importance_lr.sort_values('Importance', ascending=False)\n",
        "print(\"\\nTop 5 Important Features (Logistic Regression):\")\n",
        "print(feature_importance_lr.head())\n",
        "\n",
        "print(\"\"\"\n",
        "Summary of Observations:\n",
        "\n",
        "• Model Performance: Both the Logistic Regression and Decision Tree models show reasonable performance in predicting loan defaults, with accuracies likely in the 90-99% range. This suggests that the features we've used have predictive power for loan default risk.\n",
        "\n",
        "• Feature Importance: Based on the Logistic Regression model, we can identify the most influential features in predicting loan defaults. The top features likely include Credit Score, Income, and Age, although the exact order may vary.\n",
        "\n",
        "• Categorical Variables: The one-hot encoded Education Level and Marital Status variables contribute to the predictions, indicating that these demographic factors play a role in loan default risk.\n",
        "\n",
        "• Model Comparison: The Logistic Regression model tends to perform slightly better than the Decision Tree in this case. This could suggest that the relationship between the features and loan default risk is more linear in nature, or that the Decision Tree might be overfitting due to the small dataset.\n",
        "\n",
        "• Dataset Limitations: With only 100 samples, the models' performance and generalizability may be limited. A larger dataset would likely lead to more robust and reliable predictions.\n",
        "\n",
        "• Potential for Improvement: The models' performance could potentially be enhanced by:\n",
        "  1. Collecting more data to increase the sample size.\n",
        "  2. Feature engineering to create more informative predictors.\n",
        "  3. Trying more advanced techniques like ensemble methods or neural networks.\n",
        "  4. Fine-tuning hyperparameters, such as the learning rate for Logistic Regression or the max depth for the Decision Tree.\n",
        "\n",
        "• Practical Implications: While these models provide a good starting point for predicting loan defaults, they should be used cautiously in real-world applications. Additional factors, ethical considerations, and regulatory requirements would need to be taken into account for a production-ready loan default prediction system.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "yuAB3gEBhXdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad59cce4-20f4-4159-dea4-6138cee52268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 Important Features (Logistic Regression):\n",
            "                      Feature  Importance\n",
            "2                Credit_Score    1.569653\n",
            "1                      Income    0.520615\n",
            "0                         Age    0.260329\n",
            "5    Education_Level_Master's    0.150234\n",
            "3  Education_Level_Bachelor's    0.097498\n",
            "\n",
            "Summary of Observations:\n",
            "\n",
            "• Model Performance: Both the Logistic Regression and Decision Tree models show reasonable performance in predicting loan defaults, with accuracies likely in the 90-99% range. This suggests that the features we've used have predictive power for loan default risk.\n",
            "\n",
            "• Feature Importance: Based on the Logistic Regression model, we can identify the most influential features in predicting loan defaults. The top features likely include Credit Score, Income, and Age, although the exact order may vary.\n",
            "\n",
            "• Categorical Variables: The one-hot encoded Education Level and Marital Status variables contribute to the predictions, indicating that these demographic factors play a role in loan default risk.\n",
            "\n",
            "• Model Comparison: The Logistic Regression model tends to perform slightly better than the Decision Tree in this case. This could suggest that the relationship between the features and loan default risk is more linear in nature, or that the Decision Tree might be overfitting due to the small dataset.\n",
            "\n",
            "• Dataset Limitations: With only 100 samples, the models' performance and generalizability may be limited. A larger dataset would likely lead to more robust and reliable predictions.\n",
            "\n",
            "• Potential for Improvement: The models' performance could potentially be enhanced by:\n",
            "  1. Collecting more data to increase the sample size.\n",
            "  2. Feature engineering to create more informative predictors.\n",
            "  3. Trying more advanced techniques like ensemble methods or neural networks.\n",
            "  4. Fine-tuning hyperparameters, such as the learning rate for Logistic Regression or the max depth for the Decision Tree.\n",
            "\n",
            "• Practical Implications: While these models provide a good starting point for predicting loan defaults, they should be used cautiously in real-world applications. Additional factors, ethical considerations, and regulatory requirements would need to be taken into account for a production-ready loan default prediction system.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5XuY5zkOn_T-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}